Task Definitions • Task 1 (AND/OR): Recover wiring among only the AND/OR gates. We take all observed edges between nodes labeled “and” or “or” as positives, and randomly sample an equal number of non‐existing pairs among those same nodes as negatives. This becomes our first link‐prediction problem. • Task 2 (NAND/NOR): Same recipe, but now only for the NAND/NOR subset. We ask the model to learn the connectivity rules in this logic family. • Task 3 (XOR/XNOR): Shift to a different logical regime—exclusive‐OR style gates—and relearn which pairs should or shouldn’t be wired. • Task 4 (BUF/NOT): Finally, the buffer‐inverter domain, again sampling positive and negative pairs just among BUF/NOT nodes.

Each stage teaches the same model a new “connectivity regime” defined by that gate‐type subgraph, and we track how well it retains the earlier wiring knowledge (via EWC).

Why still use a GCN encoder? • Rich Node Embeddings: GCNs fuse each node’s own features (fan‐in, centrality, etc.) with its local neighborhood structure. For link prediction, we need embeddings that reflect both a gate’s intrinsic properties and its position in the netlist. • Parameter Sharing: All tasks share a single GCN encoder so we can apply EWC regularization to the same parameters—this lets us quantify “parameter drift” and “Fisher‐weighted drift” as the model learns each new wiring pattern. • Dot‐Product Decoder: Once we have embeddings h_u and h_v from the GCN, the simplest, most effective link score is their dot‐product (passed through a sigmoid). This avoids adding a bulky decoder and keeps the whole system lightweight and comparable to the classification setup we used before.

In short, even though we’ve moved from node‐classification to edge‐classification, the GCN remains the natural way to generate those node‐level representations you then compare to decide if two gates ought to be connected.